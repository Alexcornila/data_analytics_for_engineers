{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts in Machine Learning\n",
    "\n",
    "<center>\n",
    "<img src=\"images/overview.png\" width=\"900\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will discuss the basic structure and concepts that underly machine learning models, and explore the various types of algorithms used in machine learning through prototype examples. Ultimately the `scikit-learn` python package and other useful programming tools will be introduced. \n",
    "\n",
    "* Programming tools\n",
    "    - scikit-learn\n",
    "    - scipy\n",
    "    - autograd\n",
    "* Machine learning basics:\n",
    "    - The structure of a machine-learing model\n",
    "    - Classes of models\n",
    "    - Model complexity (hyperparameter selection)\n",
    "    - Model parameter optimization (loss/objective function)\n",
    "    - Model validation (cross validation)\n",
    "    - Interpolation vs. Extrapolation\n",
    "    - Local vs. Global models\n",
    "* Mathematical background:\n",
    "    - Linear vs. Non-linear models\n",
    "    - Linear algebra\n",
    "    - Numerical optimization\n",
    "* Prototype algorithms\n",
    "    - Generalized linear regression\n",
    "    - kNN (classification)\n",
    "    - k-means (clustering)\n",
    "    - PCA (dimensional reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming tools\n",
    "\n",
    "Machine learning algorithms can be complicated. Luckily, many implementations are openly available in Python through the `scikit-learn` package. There are [tutorials](http://scikit-learn.org/stable/tutorial/index.html) available, and plenty of pages with more [background info](https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [optimization packages](https://docs.scipy.org/doc/scipy/reference/optimize.html) in `scipy` can also be very useful, especially when implementing your own machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `autograd` package provides [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) of python functions. This is a somewhat advanced feature that is generally beyond the scope of this course, but it is a very useful tool to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np   # autograd has its own \"version\" of numpy that must be used\n",
    "from autograd import grad # the \"grad\" function provides derivatives\n",
    "\n",
    "def position(t):\n",
    "    s1 = 5\n",
    "    s2 = 12\n",
    "    s3 = 40\n",
    "    if t < 10:\n",
    "        pos = s1*t\n",
    "    elif t < 15:\n",
    "        pos = s2*t - 10*(s2-s1)\n",
    "    else:\n",
    "        pos = s3*t - 15*(s3-s2) - 10*(s2-s1)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the derivative of this function using `autograd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "t = np.linspace(0,20,100) # create a time series from 0-20 with 100 points\n",
    "x = [position(ti) for ti in t]\n",
    "speed = grad(position)\n",
    "accel = grad(speed)\n",
    "s = [speed(ti) for ti in t]\n",
    "a = [accel(ti) for ti in t]\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "axes[0].plot(t,x)\n",
    "axes[1].plot(t,s)\n",
    "axes[2].plot(t,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it is most efficient to use a model that is already implemented in `scikit-learn` (or other packages) in a practical scenario, but it is also useful to build your own versions of the algorithms to better understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of a machine-learning model\n",
    "\n",
    "The goal of machine learning is to utilize only data to \"learn\" a relationship between an input and an output:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x})$\n",
    "\n",
    "where $f$ is the model, $x$ is the model input and $y$ is the model output. The model inputs, $\\vec{x}$ are often called the **features** of a data point. Sometimes the features are easy to obtain directly from the raw data (e.g. numerical attributes like concentration, temperature, pressure, etc.), but as we will see later in the course extracting features from raw data can be a challenge (e.g. images, audio, etc.). Features are \"fingerprints\" of raw input data.\n",
    "\n",
    "Of course representing the model as $f$ is a gross oversimplification. The model needs:\n",
    "\n",
    "* **parameters**, $\\vec{W}$, that define its behavior (e.g. slope, intercept)\n",
    "* **hyperparameters**, $\\vec{\\eta}$, that define the structure of the model (e.g. the number of polynomial terms)\n",
    "\n",
    "Notably, $\\vec{W}$ will depend on $\\vec{\\eta}$. So, we can be a little more specific:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    "\n",
    "Machine learning seeks to optimize both $\\vec{W}$ (parameter optimization) and $\\vec{\\eta}$ (complexity optimization) in order to obtain a model that generalizes to new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "example = pd.read_csv('example.csv')\n",
    "example.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = example['x1']\n",
    "y = example['x2']\n",
    "c = example['class']\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x,y,c=c)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes of machine-learning models\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    "\n",
    "We can classify models based on a few criteria:\n",
    "\n",
    "#### Supervised vs. Unsupervised\n",
    "\n",
    "In a \"supervised\" model we have \"training data\" for $\\vec{y}$, meaning that there are examples to define the pattern/relationship we are looking for.\n",
    "\n",
    "In an \"unsupervised\" model $\\vec{y}$ is determined by the structure of the inputs $\\vec{x}$. This is not really intutive given the way $y=f(x)$ is written; another way of thinking about it is that we look for \"inherent\" patterns in $\\vec{x}$ and those are our \"outputs\".\n",
    "\n",
    "#### Supervised models: Classification and regression\n",
    "\n",
    "Supervised models can be classified by the nature of their outputs. If the output $\\vec{y}$ is a continuous variable then it is a **regression** model, while if it is a discrete (boolean, ordinal, integer, etc.) variable then it is a **classification** model.\n",
    "\n",
    "#### Unsupervised models: Dimensional reduction and clustering\n",
    "\n",
    "There are two main types of unsupervised learning. **Dimensional reduction** algorithms project high-dimensional (many parameters) inputs ($\\vec{x}$) to a lower-dimensional space ($\\vec{\\tilde{x}}$, where len($\\vec{\\tilde{x}}$) < len($\\vec{x}$)). **Clustering** algorithms assign labels/groups to data points based on similarity metrics. Clustering is like unsupervised classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Polynomial regression\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 + ...$\n",
    "\n",
    "$y = \\sum_i^N \\beta_i x_i^N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    return np.array([x**k for k in range(0,N+1)]).T\n",
    "    \n",
    "X = polynomial_features(x, 2)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` approach to polynomial regression is to build up a \"feature space\" of polynomial transforms of the original data, then use multi-linear regression. The common notation is to refer to the feature space as `X` and output as `y` for supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X,y) #<- \"fit\" the model (optimize the parameters beta_i)\n",
    "yhat = model.predict(X) #<- get the model predictions (evaluate the optimized function)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y) #<- plot the original data\n",
    "ax.scatter(x,yhat,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model complexity\n",
    " \n",
    " $\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    " \n",
    " The \"complexity\" of a model is defined by its hyperparameters ($\\vec{\\eta}$). The goal of machine learning is to **optimize the complexity** of a model so that it **generalizes to new examples**. In order to achieve this goal we first need a way to quantify complexity so that we can optimize it.\n",
    " \n",
    " In general there are a few strategies:\n",
    " \n",
    " * Number of parameters: \"Complexity\" varies linearly with number of parameters\n",
    " * Information criteria: \"Complexity\" varies logarithmically with number of parameters and is balanced with model accuracy.\n",
    " * \"Smoothness\": \"Complexity\" is related to the maximum curvature of the model\n",
    " \n",
    " \"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\"\n",
    " \n",
    " -- John Von Neumann\n",
    " \n",
    " (see an [example here](https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameter optimization\n",
    "\n",
    "The \"parameters\" of the model, $\\vec{W}$, must also be determined. This is achieved through numerical and/or analytical optimization of a \"loss function\" or \"objective function\" ($L(\\vec{W})$). This function defines the quality of the model.\n",
    "\n",
    "Typical loss functions include:\n",
    "\n",
    "* Sum of squared error (least-squares regression)\n",
    "* Sum of squared error plus size of parameters (regularization)\n",
    "* Mean absolute percentage error (neural networks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = sum((y-yhat)**2)\n",
    "MAE = np.sqrt(SSE/len(y))\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "\n",
    "We also need a strategy to see if our model will **generalize to new examples**. This is achieved by \"cross-validation\", where some examples (\"test\" examples) are hidden when the model is fit to \"training\" examples, and the loss function is assessed on the data that was hidden.\n",
    "\n",
    "There are many strategies for cross-validation:\n",
    "\n",
    "* hold-out: randomly leave out a percentage (usually ~30%) of the data during training.\n",
    "* k-fold: select `k` (usually 3-5) randomly-assigned sub-groups of data, and train `k` times holding each group out.\n",
    "* leave p out: leave `p` (usually 1) samples out of the training and assess the error for the `p` that were left out. Repeat for all possible `p` subsets of the sample.\n",
    "* bootstrapping: random selection with replacement to generate a sample of the same size as the original dataset, with a number of repetitions.\n",
    "\n",
    "Cross-validation is used to determine hyperparameters. In this case, even the \"test\" sets are used to optimize the model. It is common to select an additional \"validation\" or \"holdout\" subset for a final validation of the model.\n",
    "\n",
    "Important (and often violated) assumption: **The collected data is representative of future data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = 5\n",
    "X = polynomial_features(x, N)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "model.fit(X_train,y_train) #<- \"fit\" the model with training data\n",
    "yhat = model.predict(X_test) #<- get the model predictions for the test data\n",
    "\n",
    "SSE = sum((y_test-yhat)**2)\n",
    "MAE = np.sqrt(SSE/len(y)) #<- evaluate the loss function for the test data\n",
    "print('MAE (test):', MAE)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y) #<- plot the original data\n",
    "ax.scatter(X_test[:,1],yhat,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the hyperparameters of the polynomial regression model that optimize the complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation vs. Extrapolation\n",
    "\n",
    "In general, machine learning models can **only** interpolate. There are possible exceptions, but this requires some specialized model development and/or prior knowledge of the nature of the model.\n",
    "\n",
    "\"Extrapolation\" with machine learning models is typically achieved through search/exploration algorithms or \"adaptive learning\". These algorithms utilize machine-learning models to produce an iterative experimental design scheme that involves collection of new data. This effectively turns extrapolation problems into interpolation problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_extrap = np.linspace(min(x)-5, max(x)+5, 100)\n",
    "X_extrap = polynomial_features(x_extrap, N)\n",
    "\n",
    "y_extrap = modebl.predict(X_extrap) #<- try to make a prediction out of original bounds\n",
    "\n",
    "ax.scatter(X_extrap[:,1],y_extrap,color='r')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global vs. Local models\n",
    "\n",
    "A \"global\" model has parameters that do not explicitly depend on or include the input space. The polynomial regression model is an example of a global model.\n",
    "\n",
    "A \"local\" model includes some parameters or hyperparameters that are in the model inputs or defined on the domain of the independent variable. A spline model is an example of a local model.\n",
    "\n",
    "Local models are generally excellent for interpolation, but fail miseraly for extrapolation, while global models are less accurate for interpolation but provide more reasonable extrapolations. Local models tend to have many more parameters, and proper optimization of model complexity can lead to similar performance for both types.\n",
    "\n",
    "*Note:* These definitions are not standard, but I find them intuitive. More common terms for \"local\" models are \"kernel\" models or \"piecewise\" models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and non-linear models\n",
    "\n",
    "<center>\n",
    "<img src=\"images/convex.png\" width=\"300\">\n",
    "<img src=\"images/nonconvex.png\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "Linear models have a unique solution that can typically be obtained analytically and evaluated using linear algebra. Most linear models can be reduced to:\n",
    "\n",
    "$ \\underline{\\underline{A}} \\vec{x} = \\vec{b} $\n",
    "\n",
    "or, in summation notation:\n",
    "\n",
    "$ \\sum_i A_{ij} x_{i} = b_j $\n",
    "\n",
    "Non-linear models have multiple optima, and are considerably more complex. They can be solved with a range of algorithms that are often applied directly to the objective function:\n",
    "\n",
    "$\\frac{\\partial L(\\vec{W})}{\\partial \\vec{W}} = 0$\n",
    "\n",
    "Index notation can be useful for taking derivatives with respect to vectors:\n",
    "\n",
    "$\\frac{\\partial L(W_j)}{\\partial W_i} = 0$\n",
    "\n",
    "This is the \"Jacobian\" ($J_i$) of the loss function. Many optimization algorithms also use the \"Hessian\" matrix:\n",
    "\n",
    "$H_{ij} = \\frac{\\partial^2 L(W_k)}{\\partial W_i \\partial W_j}$\n",
    "\n",
    "The \"Jacobian\" and \"Hessian\" are multi-dimensional first- and second-derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is polynomial regression a \"linear\" or \"nonlinear\" model? Is it \"global\" or \"local\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on notation\n",
    "\n",
    "The use of \"index\" or \"summation\" notation is common in machine learning, and makes implementation and vector calculus much more convenient. You should be comfortable with (at least) the standard \"[vector notation](https://en.wikipedia.org/wiki/Vector_notation)\" ($\\underline{\\underline{A}}$, $\\vec{x}$) and \"[index notation](https://en.wikipedia.org/wiki/Index_notation)\" ($A_{ij}, x_i$). We will often switch between these notations, sometimes even within a single problem/derivation, so you should be comfortable with both.\n",
    "\n",
    "If you are struggling with index notation [this post](https://math.stackexchange.com/questions/2063241/matrix-multiplication-notation) may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "You should already be famililar with linear algebra, but we will briefly review the basics and show how it works in `numpy` by covering the following:\n",
    "\n",
    "Formulating your code as matrix-matrix and matrix-vector operations in Numpy will make it much more efficient. We will briefly cover syntax for:\n",
    "\n",
    "* scalar*vector\n",
    "* scalar*matrix\n",
    "* matrix*vector\n",
    "* matrix*matrix\n",
    "* inverse\n",
    "* solve Ax=b\n",
    "* eigendecomposition\n",
    "\n",
    "`numpy` notes:\n",
    "\n",
    "* reshaping and resizing arrays\n",
    "* boolean and comparison operators on arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scalar-array operations\n",
    "\n",
    "We can use the usual arithmetic operators to multiply, add, subtract, and divide arrays with scalar numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v1 = np.arange(0, 5)\n",
    "print(v1)\n",
    "print('-'*10)\n",
    "print(v1*2)\n",
    "print('-'*10)\n",
    "print(v1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Same goes for matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M = np.random.rand(2,2)\n",
    "print(M)\n",
    "print('-'*10)\n",
    "print(M*2)\n",
    "print('-'*10)\n",
    "print(M+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Element-wise array-array operations\n",
    "\n",
    "When we add, subtract, multiply and divide arrays with each other, the default behaviour is **element-wise** operations. This is different from Matlab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v1 = np.arange(2,6)\n",
    "print(v1)\n",
    "print(v1*v1)\n",
    "print(v1/v1)\n",
    "\n",
    "print('-'*10)\n",
    "\n",
    "M = np.array([[1,2],[3,4]])\n",
    "print(M)\n",
    "print(M*M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix algebra\n",
    "\n",
    "What about matrix mutiplication?\n",
    "\n",
    "* use the `dot` function (recommended)\n",
    "* use the `matrix` class (`+`, `*`, `-` use matrix algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.eye(3,3)\n",
    "v = np.array([1,2,3])\n",
    "print(np.dot(A,v))\n",
    "print(np.dot(A,A))\n",
    "print(np.dot(v,v))\n",
    "\n",
    "A = np.matrix(A)\n",
    "v = np.matrix(v)\n",
    "print(A*v.T)\n",
    "print(A*A)\n",
    "print(v*v.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common matrix operations\n",
    "\n",
    "We can easily calculate the inverse and determinant using `inv` and `det`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[-1,2],[3,-1]])\n",
    "print(A)\n",
    "print(np.linalg.inv(A))\n",
    "print(np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear equation systems\n",
    "\n",
    "Linear equation systems on the matrix form\n",
    "\n",
    "$\\underline{\\underline{A}} \\vec{x} = \\vec{b}$\n",
    "\n",
    "where $A$ is a matrix and $x,b$ are vectors can be solved like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.linalg  import solve\n",
    "\n",
    "N = 3\n",
    "A = np.random.rand(N,N)\n",
    "b = np.random.rand(N)\n",
    "\n",
    "x = solve(A, b)\n",
    "\n",
    "print(x)\n",
    "# check\n",
    "np.dot(A, x) - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same with\n",
    "\n",
    "$\\underline{\\underline{A}} \\underline{\\underline{X}} = \\underline{\\underline{B}}$\n",
    "\n",
    "where $A, B, X$ are matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(N,N)\n",
    "B = np.random.rand(N,N)\n",
    "\n",
    "X = solve(A, B)\n",
    "# check\n",
    "np.dot(A, X) - B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Convince yourself that $\\underline{\\underline{A}} \\underline{\\underline{X}} = \\underline{\\underline{B}}$ is equivalent to solving `N` independent systems of the form $\\underline{\\underline{A}} \\vec{x} = \\vec{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and eigenvectors\n",
    "\n",
    "The eigenvalue problem for a matrix $\\underline{\\underline{A}}$:\n",
    "\n",
    "$\\underline{\\underline{A}} v_n = \\lambda_n v_n$\n",
    "\n",
    "where $v_n$ is the $n$th eigenvector and $\\lambda_n$ is the $n$th eigenvalue.\n",
    "\n",
    "To calculate eigenvalues of a matrix, use the `eigvals` function, and for calculating both eigenvalues and eigenvectors, use the function `eig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvals, eig\n",
    "evals = eigvals(A)\n",
    "print(evals)\n",
    "\n",
    "evals, evecs = eig(A)\n",
    "print(evals)\n",
    "print(evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors corresponding to the $n$th eigenvalue (stored in evals[n]) is the $n$th column in evecs, i.e., evecs[:,n]. To verify this, let's try mutiplying eigenvectors with the matrix and compare to the product of the eigenvector and the eigenvalue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "\n",
    "np.dot(A, evecs[:,n]) - evals[n] * evecs[:,n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Eigendecompositions are very common in machine learning and data analysis, so you should brush up on this if you have forgotten what it means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numpy notes:\n",
    "\n",
    "### Reshaping and resizing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shape of an Numpy array can be modified without copying the underlaying data, which makes it a fast operation even for large arrays. There are rules that govern how this reshaping takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R = np.random.rand(3,3,3)\n",
    "print(R.shape)\n",
    "n,m,p = R.shape\n",
    "Q = R.reshape((n, m*p))\n",
    "print(Q.shape)\n",
    "F = R.flatten() #the \"flatten\" function turns the whole array into a vector\n",
    "print(F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two common pitfalls in reshaping arrays:\n",
    "\n",
    "* Reshaping rules do not behave as expected\n",
    "* Reshaping provides a different \"view\" of the data, but **does not copy it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(R[0,0,0])\n",
    "print(F[0])\n",
    "print(R[0,1,0])\n",
    "print(F[1])\n",
    "print(F[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(R[0,0,0])\n",
    "Q[0] = 10\n",
    "print(R[0,0,0]) #resize does not copy the data\n",
    "F[0] = 6\n",
    "print(R[0,0,0]) #flatten makes copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Making a \"deep copy\"\n",
    "\n",
    "If you really want a copy of an array, use the `np.copy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "print(A)\n",
    "B = A\n",
    "B[0,0] = 10\n",
    "print(A)\n",
    "Acopy = np.copy(A)\n",
    "Acopy[1,1] = 6\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization (finding minima or maxima of a function) is a large field in mathematics, and optimization of complicated functions or in many variables can be rather involved. Here we will only look at a few very simple cases. For a more detailed introduction to optimization with SciPy see: http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html\n",
    "\n",
    "We can typically achieve good results using `scipy`'s built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4*x**3 + (x-2)**2 + x**4\n",
    "\n",
    "fig, ax  = plt.subplots()\n",
    "x = np.linspace(-5, 3, 100)\n",
    "ax.plot(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of optimizers available. We will use the common BFGS and CG optimizers here, but you can read more in the [documentation](https://docs.scipy.org/doc/scipy/reference/optimize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.optimize  import minimize\n",
    "\n",
    "x_min = minimize(f, 0.5, method='BFGS')\n",
    "x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related problem is solving an equation, which can be achieved with the `fsolve` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return np.sin(3*x)*(1/x)\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(10,4))\n",
    "x = np.linspace(1, 10, 100)\n",
    "ax.plot(x, g(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "ans = fsolve(g, 0)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype algorithms\n",
    "\n",
    "We will introduce a prototype for each class of algorithm with a derivation and/or custom implementation. These algorithms will be revisited later in the course (along with others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k means (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (dimensional reduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
